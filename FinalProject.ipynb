{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceedc51-b906-4aad-ae24-922401a8a6ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CalCOFI\n",
    "### Over 60 years of oceanographic data\n",
    "#### Dataset: https://www.kaggle.com/datasets/sohier/calcofi\n",
    "#### Table Info: https://calcofi.org/data/oceanographic-data/bottle-database/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acbab4-a9db-4185-acc5-ca30d8529331",
   "metadata": {},
   "source": [
    "## EDA and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff18d45-0aa8-4e1a-8206-124a541b2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f2d50-db24-4988-852b-803f9f4c1cb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157f41db-c8b0-4f58-b55c-ea8902313802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/spark-3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-05-22 00:53:27,037 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Loading in spark '''\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    ('spark.master', 'local[1]'), \n",
    "    ('spark.app.name', 'App Name')])\n",
    "    \n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f6f5c2-1440-4cf2-9cae-ab3c320a4a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 00:53:39,297 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 864863 rows in the initial dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "''' Read the data '''\n",
    "# bottle = spark.read.csv(\"hdfs:///bottle.csv\", header=True, inferSchema=True).cache() ##leslie and katie's path\n",
    "bottle = spark.read.csv(\"file:///home/work/Final/bottle.csv\", header=True, inferSchema=True).cache() ##karina's path\n",
    "maxRows = bottle.count()\n",
    "print(\"There are\", maxRows, \"rows in the initial dataframe\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90183886-9199-44f7-9663-a89153669e20",
   "metadata": {},
   "source": [
    "cast = spark.read.csv(\"file:///home/work/Final/cast.csv\", header=True, inferSchema=True).cache()\n",
    "cast.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b549a1-17a7-4e3c-b00e-44d029c5651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74 columns in the initial dataframe\n",
      "root\n",
      " |-- Cst_Cnt: integer (nullable = true)\n",
      " |-- Btl_Cnt: integer (nullable = true)\n",
      " |-- Sta_ID: string (nullable = true)\n",
      " |-- Depth_ID: string (nullable = true)\n",
      " |-- Depthm: integer (nullable = true)\n",
      " |-- T_degC: double (nullable = true)\n",
      " |-- Salnty: double (nullable = true)\n",
      " |-- O2ml_L: double (nullable = true)\n",
      " |-- STheta: double (nullable = true)\n",
      " |-- O2Sat: double (nullable = true)\n",
      " |-- Oxy_µmol/Kg: double (nullable = true)\n",
      " |-- BtlNum: integer (nullable = true)\n",
      " |-- RecInd: integer (nullable = true)\n",
      " |-- T_prec: integer (nullable = true)\n",
      " |-- T_qual: integer (nullable = true)\n",
      " |-- S_prec: integer (nullable = true)\n",
      " |-- S_qual: integer (nullable = true)\n",
      " |-- P_qual: integer (nullable = true)\n",
      " |-- O_qual: integer (nullable = true)\n",
      " |-- SThtaq: integer (nullable = true)\n",
      " |-- O2Satq: integer (nullable = true)\n",
      " |-- ChlorA: double (nullable = true)\n",
      " |-- Chlqua: integer (nullable = true)\n",
      " |-- Phaeop: double (nullable = true)\n",
      " |-- Phaqua: integer (nullable = true)\n",
      " |-- PO4uM: double (nullable = true)\n",
      " |-- PO4q: integer (nullable = true)\n",
      " |-- SiO3uM: double (nullable = true)\n",
      " |-- SiO3qu: integer (nullable = true)\n",
      " |-- NO2uM: double (nullable = true)\n",
      " |-- NO2q: integer (nullable = true)\n",
      " |-- NO3uM: double (nullable = true)\n",
      " |-- NO3q: integer (nullable = true)\n",
      " |-- NH3uM: double (nullable = true)\n",
      " |-- NH3q: integer (nullable = true)\n",
      " |-- C14As1: double (nullable = true)\n",
      " |-- C14A1p: integer (nullable = true)\n",
      " |-- C14A1q: integer (nullable = true)\n",
      " |-- C14As2: double (nullable = true)\n",
      " |-- C14A2p: integer (nullable = true)\n",
      " |-- C14A2q: integer (nullable = true)\n",
      " |-- DarkAs: double (nullable = true)\n",
      " |-- DarkAp: integer (nullable = true)\n",
      " |-- DarkAq: integer (nullable = true)\n",
      " |-- MeanAs: double (nullable = true)\n",
      " |-- MeanAp: integer (nullable = true)\n",
      " |-- MeanAq: integer (nullable = true)\n",
      " |-- IncTim: string (nullable = true)\n",
      " |-- LightP: double (nullable = true)\n",
      " |-- R_Depth: double (nullable = true)\n",
      " |-- R_TEMP: double (nullable = true)\n",
      " |-- R_POTEMP: double (nullable = true)\n",
      " |-- R_SALINITY: double (nullable = true)\n",
      " |-- R_SIGMA: double (nullable = true)\n",
      " |-- R_SVA: double (nullable = true)\n",
      " |-- R_DYNHT: double (nullable = true)\n",
      " |-- R_O2: double (nullable = true)\n",
      " |-- R_O2Sat: double (nullable = true)\n",
      " |-- R_SIO3: double (nullable = true)\n",
      " |-- R_PO4: double (nullable = true)\n",
      " |-- R_NO3: double (nullable = true)\n",
      " |-- R_NO2: double (nullable = true)\n",
      " |-- R_NH4: double (nullable = true)\n",
      " |-- R_CHLA: double (nullable = true)\n",
      " |-- R_PHAEO: double (nullable = true)\n",
      " |-- R_PRES: integer (nullable = true)\n",
      " |-- R_SAMP: integer (nullable = true)\n",
      " |-- DIC1: double (nullable = true)\n",
      " |-- DIC2: double (nullable = true)\n",
      " |-- TA1: double (nullable = true)\n",
      " |-- TA2: double (nullable = true)\n",
      " |-- pH2: double (nullable = true)\n",
      " |-- pH1: double (nullable = true)\n",
      " |-- DIC Quality Comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' View the inferred schema '''\n",
    "print(\"There are\", len(bottle.columns), \"columns in the initial dataframe\")\n",
    "bottle.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee33de5-2262-4daa-b0d9-7a8e6a8f88a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9c310a-7bc4-42ed-9e77-08c50a638897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+--------------------+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+-------------------+\n",
      "|Cst_Cnt|Btl_Cnt|     Sta_ID|            Depth_ID|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|BtlNum|RecInd|T_prec|T_qual|S_prec|S_qual|P_qual|O_qual|SThtaq|O2Satq|ChlorA|Chlqua|Phaeop|Phaqua|PO4uM|PO4q|SiO3uM|SiO3qu|NO2uM|NO2q|NO3uM|NO3q|NH3uM|NH3q|C14As1|C14A1p|C14A1q|C14As2|C14A2p|C14A2q|DarkAs|DarkAp|DarkAq|MeanAs|MeanAp|MeanAq|IncTim|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2| TA1| TA2| pH2| pH1|DIC Quality Comment|\n",
      "+-------+-------+-----------+--------------------+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+-------------------+\n",
      "|      1|      1|054.0 056.0|19-4903CR-HY-060-...|     0|  10.5| 33.44|  null|25.649| null|       null|  null|     3|     1|  null|     2|  null|     9|     9|  null|     9|  null|     9|  null|     9| null|   9|  null|     9| null|   9| null|   9| null|   9|  null|  null|     9|  null|  null|     9|  null|  null|     9|  null|  null|     9|  null|  null|    0.0|  10.5|    10.5|     33.44|  25.64|233.0|    0.0|null|   null|  null| null| null| null| null|  null|   null|     0|  null|null|null|null|null|null|null|               null|\n",
      "|      1|      2|054.0 056.0|19-4903CR-HY-060-...|     8| 10.46| 33.44|  null|25.656| null|       null|  null|     3|     2|  null|     2|  null|     9|     9|  null|     9|  null|     9|  null|     9| null|   9|  null|     9| null|   9| null|   9| null|   9|  null|  null|     9|  null|  null|     9|  null|  null|     9|  null|  null|     9|  null|  null|    8.0| 10.46|   10.46|     33.44|  25.65|232.5|   0.01|null|   null|  null| null| null| null| null|  null|   null|     8|  null|null|null|null|null|null|null|               null|\n",
      "+-------+-------+-----------+--------------------+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' See a snippet of what this dataframe looks like '''\n",
    "bottle.show(2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d731aee0-1092-48a2-bd5e-dc2cb32fbbd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "cast.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd0c64-b9f9-4bc2-9ca0-43c52572cd5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove columns that are not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9298c03-b355-4f8c-b8c1-3637564128c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Removing the four string columns because they aren't useful for our purposes\n",
    "* Sta_ID: Line and Station\n",
    "* Depth_ID: Uses the Cast_ID prefix ([Century]-[Year][Month][ShipCode]-[CastType][Julian Day]-[CastTime]-[Line][Sta]) but adds three additional variables: [Depth][Bottle]-[Rec_Ind]\n",
    "* IncTim: Elapsed incubation time of the primary productivity experiment\n",
    "* DIC Quality Comment: Quality Comment\n",
    "\n",
    "Also removing the Cast and Bottle counts, which are essentially indexes (identifiers)\n",
    "* 'Cst_Cnt': Auto-numbered Cast Count - all casts consecutively numbered. 1 is first station done\n",
    "* 'Btl_Cnt': Auto-numbered Bottle count- all bottles ever sampled, consecutively numbered\n",
    "* 'BtlNum': Bottle Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb638b3-e274-43c1-9ccb-10556b29e96c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 67 columns and 864863 rows\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|T_qual|S_prec|S_qual|P_qual|O_qual|SThtaq|O2Satq|ChlorA|Chlqua|Phaeop|Phaqua|PO4uM|PO4q|SiO3uM|SiO3qu|NO2uM|NO2q|NO3uM|NO3q|NH3uM|NH3q|C14As1|C14A1p|C14A1q|C14As2|C14A2p|C14A2q|DarkAs|DarkAp|DarkAq|MeanAs|MeanAp|MeanAq|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1 |TA2 |pH2 |pH1 |\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+\n",
      "|0     |10.5  |33.44 |null  |25.649|null |null       |3     |1     |null  |2     |null  |9     |9     |null  |9     |null  |9     |null  |9     |null |9   |null  |9     |null |9   |null |9   |null |9   |null  |null  |9     |null  |null  |9     |null  |null  |9     |null  |null  |9     |null  |0.0    |10.5  |10.5    |33.44     |25.64  |233.0|0.0    |null|null   |null  |null |null |null |null |null  |null   |0     |null  |null|null|null|null|null|null|\n",
      "|8     |10.46 |33.44 |null  |25.656|null |null       |3     |2     |null  |2     |null  |9     |9     |null  |9     |null  |9     |null  |9     |null |9   |null  |9     |null |9   |null |9   |null |9   |null  |null  |9     |null  |null  |9     |null  |null  |9     |null  |null  |9     |null  |8.0    |10.46 |10.46   |33.44     |25.65  |232.5|0.01   |null|null   |null  |null |null |null |null |null  |null   |8     |null  |null|null|null|null|null|null|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+----+------+------+-----+----+-----+----+-----+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Dropping unneeded columns and viewing two rows of the resulting dataframe '''\n",
    "deleteList1 = [\"Sta_ID\",\"Depth_ID\",\"IncTim\",\"DIC Quality Comment\",\"Cst_Cnt\",\"Btl_Cnt\",\"BtlNum\"]\n",
    "bottle = bottle.drop(*deleteList1)\n",
    "\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19572ed7-aa1b-49f4-ac37-5dd68db3a20c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Handle quality values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d923432-39d4-48f2-a991-c38d8c750408",
   "metadata": {},
   "source": [
    "deleteList2 = ['T_qual','S_qual','P_qual','O_qual','O2Satq','Chlqua','Phaeop','Phaqua','PO4uM','PO4q','SiO3qu','NO2q','NO3q','NH3q','C14A1q','C14A2q','DarkAq','MeanAq']\n",
    "for i in deleteList2:\n",
    "    numBadData = bottle.filter(bottle[i] == 8).count()\n",
    "    print(f\"Number of bad quality data for {i}: {numBadData}\")\n",
    "\n",
    "''' Dropping bad quality rows of quality columns'''\n",
    "for i in deleteList2:\n",
    "    bottle = bottle.filter(bottle[i] != 8)Removing the four columns indicating quality codes because we're using the quantity measurements instead\n",
    "* T_qual: Temperature Quality Code\n",
    "* S_qual: Salinity Quality Code\n",
    "* P_qual: Pressure Quality Code\n",
    "* O_qual: Oxygen Quality Code\n",
    "* 'O2Satq': Oxygen Saturation Quality Code\n",
    "* 'Chlqua': Chlorophyll-a Quality Code\n",
    "* 'Phaeop': Phaeophytin Quality Code\n",
    "* 'Phaqua': Phosphate Quality Code\n",
    "* 'PO4uM': Salinity Quality Code\n",
    "* 'PO4q': Phosphate Quality Code\n",
    "* 'SiO3qu': Quality Code\n",
    "* 'NO2q': Quality Code\n",
    "* 'NO3q': Nitrate Quality Code\n",
    "* 'NH3q': Ammonium Quality Code\n",
    "* 'C14A1q': 14C As1 Quality Code\n",
    "* 'C14A2q': 14C As2 Quality Code\n",
    "* 'DarkAq': 14C Assimilation Dark Bottle Quality Code\n",
    "* 'MeanAq': Mean 14C Assimilation Quality Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16d3f7d-78a9-4055-b131-91d3e178c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad quality data for T_qual: 575\n",
      "Number of bad quality data for S_qual: 1825\n",
      "Number of bad quality data for P_qual: 0\n",
      "Number of bad quality data for O_qual: 1455\n",
      "Number of bad quality data for O2Satq: 2657\n",
      "Number of bad quality data for Chlqua: 97\n",
      "Number of bad quality data for Phaeop: 0\n",
      "Number of bad quality data for Phaqua: 100\n",
      "Number of bad quality data for PO4uM: 0\n",
      "Number of bad quality data for PO4q: 302\n",
      "Number of bad quality data for SiO3qu: 155\n",
      "Number of bad quality data for NO2q: 2210\n",
      "Number of bad quality data for NO3q: 2257\n",
      "Number of bad quality data for NH3q: 0\n",
      "Number of bad quality data for C14A1q: 7\n",
      "Number of bad quality data for C14A2q: 7\n",
      "Number of bad quality data for DarkAq: 7\n",
      "Number of bad quality data for MeanAq: 7\n",
      "22552\n",
      "19286\n",
      "19046\n",
      "7692\n",
      "5577\n",
      "2919\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "deleteList2 = ['T_qual','S_qual','P_qual','O_qual','O2Satq','Chlqua','Phaeop','Phaqua','PO4uM','PO4q','SiO3qu','NO2q','NO3q','NH3q','C14A1q','C14A2q','DarkAq','MeanAq']\n",
    "for i in deleteList2:\n",
    "    numBadData = bottle.filter(bottle[i] == 8).count()\n",
    "    print(f\"Number of bad quality data for {i}: {numBadData}\")\n",
    "\n",
    "''' Dropping bad quality rows of quality columns'''\n",
    "for i in deleteList2:\n",
    "    bottle = bottle.filter(bottle[i] != 8)\n",
    "    print(bottle.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4133d539-b5c4-40e5-bd19-f832f93d548b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 49 columns and 0 rows\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|S_prec|SThtaq|ChlorA|SiO3uM|NO2uM|NO3uM|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Dropping quality/irrelevant columns and viewing two rows of the resulting dataframe '''\n",
    "bottle = bottle.drop(*deleteList2)\n",
    "\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62375da2-864f-4965-a91d-17f5a6bb64bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove columns with low data count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6515f03-4e87-44f3-8a90-19d8e0da5877",
   "metadata": {},
   "source": [
    "We start by counting the number of NaNs and nulls in each column and reporting them in a new dataframe. Then the columns with less than 200,000 non-nulls are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e938fa-d0dd-4b66-a91b-9455da02f343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|S_prec|SThtaq|ChlorA|SiO3uM|NO2uM|NO3uM|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|     0|     0|     0|     0|     0|    0|          0|     0|     0|     0|     0|     0|     0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|      0|     0|       0|         0|      0|    0|      0|   0|      0|     0|    0|    0|    0|    0|     0|      0|     0|     0|   0|   0|  0|  0|  0|  0|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Counting the number of null/NaN rows per column and outputting that in a new dataframe '''\n",
    "\n",
    "def getNullCounts(df):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in bottle.columns])\n",
    "\n",
    "nullCounter = getNullCounts(bottle)\n",
    "nullCounter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430a2382-c5b9-4bbb-8e22-5492374522e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 49 columns and 0 rows\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|S_prec|SThtaq|ChlorA|SiO3uM|NO2uM|NO3uM|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Deleting columns with less than 200000 non-nulls '''\n",
    "thresh = maxRows - 200000\n",
    "deleteList3 = []\n",
    "for value in nullCounter.columns:\n",
    "    if nullCounter.filter(nullCounter[value] > thresh).select(nullCounter[value]).collect():\n",
    "        deleteList3.append(value)\n",
    "bottle = bottle.drop(*deleteList3)\n",
    "\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde08e1-432b-4709-9333-8fd25af5f2b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove null values from chlorophyll column\n",
    "\n",
    "Because this is the target column, we can only use the non-null rows. We also get rid of the duplicate chlorophyll column `ChlorA`, keeping `R_CHLA` because it has less non-null rows (though only by a few)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98682bb-1025-43c0-81af-2cc73a7976f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Illustrating that we have two target columns that are essentially duplicates of each other '''\n",
    "bottle.corr(\"ChlorA\",\"R_CHLA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328e6c2-b8ab-4925-91ef-2b9dcda91f28",
   "metadata": {},
   "source": [
    "Because we only need one of these columns, we can delete the other. Then we drop all null rows of the R_CHLA column since it's our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0277500-8de6-4881-a7e8-54c87554155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|ChlorA|R_CHLA|\n",
      "+------+------+\n",
      "|     0|     0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Looking at which of the two target columns have more NaNs in order to select which to delete '''\n",
    "nullCounter.select(*[\"ChlorA\",\"R_CHLA\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab54d498-ec72-4f2d-a48a-3eeaec9ca27c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 48 columns and 0 rows\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|S_prec|SThtaq|SiO3uM|NO2uM|NO3uM|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Delete the ChlorA column and drop all NaN rows in the R_CHLA column '''\n",
    "bottle = bottle.drop(\"ChlorA\")\n",
    "bottle = bottle.dropna(subset=\"R_CHLA\")\n",
    "\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd094f-0bab-4514-a326-4f022ba8bcd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove \"duplicate\" columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006a902-9bae-4028-ad1e-d101de2c155d",
   "metadata": {},
   "source": [
    "As we can see from the column above, there are some columns that are essentially duplicates of each other but in different units of measurement. Of the remaining columns, here are their descriptions that will help us determine which features to compare for potential deletion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee57ca67-892e-4b86-a9f6-d1f802d3f6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Depthm', 'T_degC', 'Salnty', 'O2ml_L', 'STheta', 'O2Sat', 'Oxy_µmol/Kg', 'RecInd', 'T_prec', 'S_prec', 'SThtaq', 'SiO3uM', 'NO2uM', 'NO3uM', 'NH3uM', 'C14As1', 'C14A1p', 'C14As2', 'C14A2p', 'DarkAs', 'DarkAp', 'MeanAs', 'MeanAp', 'LightP', 'R_Depth', 'R_TEMP', 'R_POTEMP', 'R_SALINITY', 'R_SIGMA', 'R_SVA', 'R_DYNHT', 'R_O2', 'R_O2Sat', 'R_SIO3', 'R_PO4', 'R_NO3', 'R_NO2', 'R_NH4', 'R_CHLA', 'R_PHAEO', 'R_PRES', 'R_SAMP', 'DIC1', 'DIC2', 'TA1', 'TA2', 'pH2', 'pH1']\n"
     ]
    }
   ],
   "source": [
    "''' See what columns we still have '''\n",
    "print(bottle.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616a86b-f8a9-4ff9-999e-153f2066520b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Chlorophyll\n",
    "* 'ChlorA': Acetone extracted chlorophyll-a measured fluorometrically\n",
    "* 'R_CHLA': Reported Chlorophyll-a (micrograms per liter)\n",
    "\n",
    "Depth\n",
    "* 'Depthm': Depth in meters\n",
    "* 'R_Depth': Reported Depth (from pressure) in meters\n",
    "\n",
    "Water density\n",
    "* 'STheta': Potential Density of Water\n",
    "* 'R_SIGMA': Reported Potential Density of water\n",
    "\n",
    "Silicate\n",
    "* 'SiO3uM': Micromoles Silicate per liter of seawater\n",
    "* 'R_SIO3': Reported Silicate Concentration\n",
    "\n",
    "Nitrite\n",
    "* 'NO2uM': Micromoles Nitrite per liter of seawater\n",
    "* 'R_NO2': Reported Nitrite Concentration\n",
    "\n",
    "Nitrate\n",
    "* 'NO3uM': Micromoles Nitrate per liter of seawater\n",
    "* 'R_NO3': Reported Nitrate Concentration\n",
    "\n",
    "Salinity\n",
    "* 'Salnty': Practical Salinity Scale, 1978 (UNESCO, 1981a); Salinity of water\n",
    "* 'R_SALINITY': Reported Salinity (from Specific Volume Anomoly, M³/Kg)\n",
    "\n",
    "O2 saturation\n",
    "* 'O2Sat': Percent Saturation; Oxygen Saturation\n",
    "* 'R_O2Sat': Percent\tReported Oxygen Saturation\n",
    "\n",
    "Oxygen\n",
    "* 'O2ml_L': Oxygen in mL/L; Milliliters of dissolved oxygen per Liter seawater\n",
    "* 'Oxy_µmol/Kg': Oxygen in micro moles per kilogram of seawater\n",
    "* 'R_O2': Reported milliliters of oxygen per liter of seawater\n",
    "\n",
    "Temperature\n",
    "* 'T_degC': Temperature of Water\n",
    "* 'R_TEMP': Reported Temperature (Celsius)\n",
    "* 'R_POTEMP': Reported Potential Temperature (Celsius)\n",
    "\n",
    "Other\n",
    "* 'S_prec': Salinity Units of Precision\n",
    "* 'T_prec': Temperature Units of Precision\n",
    "* 'RecInd': Record Indicator\n",
    "* 'R_SVA': Reported Specific Volume Anomaly\n",
    "* 'R_DYNHT': Reported Dynamic Height\n",
    "* 'R_PO4': Reported Phosphate Concentration\n",
    "* 'R_PHAEO': Reported Phaeophytin\n",
    "* 'R_PRES': Pressure in decibars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a059ea58-211f-4a60-a723-03b49f2e352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Setting up a function that will streamline the comparing process '''\n",
    "nullCounter = getNullCounts(bottle)\n",
    "\n",
    "def psudoDuplicateCheck(features):\n",
    "    nullCounter.select(*features).show()\n",
    "\n",
    "    if len(features)==2:\n",
    "        print(\"correlation:\", bottle.corr(features[0],features[1]))\n",
    "    elif len(features)==3:\n",
    "        print(\"1&2 correlation:\", bottle.corr(features[0],features[1]))\n",
    "        print(\"2&3 correlation:\", bottle.corr(features[1],features[2]))\n",
    "        print(\"1&3 correlation:\", bottle.corr(features[0],features[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf42d601-24b6-46d8-b169-faef5e0c296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|Depthm|T_degC|Salnty|O2ml_L|STheta|O2Sat|Oxy_µmol/Kg|RecInd|T_prec|S_prec|SThtaq|SiO3uM|NO2uM|NO3uM|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_POTEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|     0|     0|     0|     0|     0|    0|          0|     0|     0|     0|     0|     0|    0|    0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|      0|     0|       0|         0|      0|    0|      0|   0|      0|     0|    0|    0|    0|    0|     0|      0|     0|     0|   0|   0|  0|  0|  0|  0|\n",
      "+------+------+------+------+------+-----+-----------+------+------+------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+-------+------+--------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nullCounter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4da4ea0d-6e66-4001-810f-39161778f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depths NaN count:\n",
      "+------+-------+\n",
      "|Depthm|R_Depth|\n",
      "+------+-------+\n",
      "|     0|      0|\n",
      "+------+-------+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Water density NaN count:\n",
      "+------+-------+\n",
      "|STheta|R_SIGMA|\n",
      "+------+-------+\n",
      "|     0|      0|\n",
      "+------+-------+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Silicate NaN count:\n",
      "+------+------+\n",
      "|SiO3uM|R_SIO3|\n",
      "+------+------+\n",
      "|     0|     0|\n",
      "+------+------+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Nitrite NaN count:\n",
      "+-----+-----+\n",
      "|NO2uM|R_NO2|\n",
      "+-----+-----+\n",
      "|    0|    0|\n",
      "+-----+-----+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Nitrate NaN count:\n",
      "+-----+-----+\n",
      "|NO3uM|R_NO3|\n",
      "+-----+-----+\n",
      "|    0|    0|\n",
      "+-----+-----+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Salinity NaN count:\n",
      "+------+----------+\n",
      "|Salnty|R_SALINITY|\n",
      "+------+----------+\n",
      "|     0|         0|\n",
      "+------+----------+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "O2 Saturation NaN count:\n",
      "+-----+-------+\n",
      "|O2Sat|R_O2Sat|\n",
      "+-----+-------+\n",
      "|    0|      0|\n",
      "+-----+-------+\n",
      "\n",
      "correlation: nan\n",
      "\n",
      "Oxygen NaN count:\n",
      "+------+-----------+----+\n",
      "|O2ml_L|Oxy_µmol/Kg|R_O2|\n",
      "+------+-----------+----+\n",
      "|     0|          0|   0|\n",
      "+------+-----------+----+\n",
      "\n",
      "1&2 correlation: nan\n",
      "2&3 correlation: nan\n",
      "1&3 correlation: nan\n",
      "\n",
      "Temperature NaN count:\n",
      "+------+------+--------+\n",
      "|T_degC|R_TEMP|R_POTEMP|\n",
      "+------+------+--------+\n",
      "|     0|     0|       0|\n",
      "+------+------+--------+\n",
      "\n",
      "1&2 correlation: nan\n",
      "2&3 correlation: nan\n",
      "1&3 correlation: nan\n"
     ]
    }
   ],
   "source": [
    "Depths = [\"Depthm\",\"R_Depth\"]\n",
    "print(\"Depths NaN count:\")\n",
    "psudoDuplicateCheck(Depths)\n",
    "\n",
    "WD = [\"STheta\",\"R_SIGMA\"]\n",
    "print(\"\\nWater density NaN count:\")\n",
    "psudoDuplicateCheck(WD)\n",
    "\n",
    "Silicate = [\"SiO3uM\",\"R_SIO3\"]\n",
    "print(\"\\nSilicate NaN count:\")\n",
    "psudoDuplicateCheck(Silicate)\n",
    "\n",
    "Nitrite = [\"NO2uM\",\"R_NO2\"]\n",
    "print(\"\\nNitrite NaN count:\")\n",
    "psudoDuplicateCheck(Nitrite)\n",
    "\n",
    "Nitrate = [\"NO3uM\",\"R_NO3\"]\n",
    "print(\"\\nNitrate NaN count:\")\n",
    "psudoDuplicateCheck(Nitrate)\n",
    "\n",
    "Salinity = [\"Salnty\",\"R_SALINITY\"]\n",
    "print(\"\\nSalinity NaN count:\")\n",
    "psudoDuplicateCheck(Salinity)\n",
    "\n",
    "Saturation = [\"O2Sat\",\"R_O2Sat\"]\n",
    "print(\"\\nO2 Saturation NaN count:\")\n",
    "psudoDuplicateCheck(Saturation)\n",
    "\n",
    "Oxygen = [\"O2ml_L\",\"Oxy_µmol/Kg\",\"R_O2\"]\n",
    "print(\"\\nOxygen NaN count:\")\n",
    "psudoDuplicateCheck(Oxygen)\n",
    "\n",
    "Temperature = [\"T_degC\",\"R_TEMP\",\"R_POTEMP\"]\n",
    "print(\"\\nTemperature NaN count:\")\n",
    "psudoDuplicateCheck(Temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abd83d-0818-45b5-94f4-076b71e4da30",
   "metadata": {},
   "source": [
    "We now delete the columns with fewer null if they have a correlation coeffecients of 0.97 or higher.\n",
    "\n",
    "In the case of a tie, we can refer to the pattern that is very apparent in no-tie cases, which is that the \"reported\" (columns starting with `R_`) have the higher non-null count. Therefore if two columns have equal null counts and high enough correlation, we delete the not-\"reported\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d1ec82-518a-485d-bce3-87978142b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 37 columns and 0 rows\n",
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|RecInd|T_prec|S_prec|SThtaq|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Deleting columns that are duplicates of some other column '''\n",
    "deleteList4 = [\"Depthm\",\"STheta\",\"SiO3uM\",\"NO2uM\",\"NO3uM\",\"Salnty\",\"O2Sat\",\"O2ml_L\",\"Oxy_µmol/Kg\",\"T_degC\",\"R_POTEMP\"]\n",
    "bottle = bottle.drop(*deleteList4)\n",
    "\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61dcfca-9ff6-4c2e-bf4f-b0185b921dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|RecInd|T_prec|S_prec|SThtaq|NH3uM|C14As1|C14A1p|C14As2|C14A2p|DarkAs|DarkAp|MeanAs|MeanAp|LightP|R_Depth|R_TEMP|R_SALINITY|R_SIGMA|R_SVA|R_DYNHT|R_O2|R_O2Sat|R_SIO3|R_PO4|R_NO3|R_NO2|R_NH4|R_CHLA|R_PHAEO|R_PRES|R_SAMP|DIC1|DIC2|TA1|TA2|pH2|pH1|\n",
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "|     0|     0|     0|     0|    0|     0|     0|     0|     0|     0|     0|     0|     0|     0|      0|     0|         0|      0|    0|      0|   0|      0|     0|    0|    0|    0|    0|     0|      0|     0|     0|   0|   0|  0|  0|  0|  0|\n",
      "+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+-------+------+----------+-------+-----+-------+----+-------+------+-----+-----+-----+-----+------+-------+------+------+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tbd\n",
    "getNullCounts(bottle).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83810ea4-8911-4692-b975-886728dd5a76",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35759665-9e24-4222-b2ed-c37722216abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Using the function from PA3 '''\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "def fill_na(df, strategy):    \n",
    "    imputer = Imputer(\n",
    "        strategy=strategy,\n",
    "        inputCols=df.columns, \n",
    "        outputCols=[\"{}_imputed\".format(c) for c in df.columns]\n",
    "    )\n",
    "    \n",
    "    new_df = imputer.fit(df).transform(df)\n",
    "    \n",
    "    ''' Select the newly created columns with all filled values '''\n",
    "    new_df = new_df.select([c for c in new_df.columns if \"imputed\" in c])\n",
    "    \n",
    "    for col in new_df.columns:\n",
    "        new_df = new_df.withColumnRenamed(col, col.split(\"_imputed\")[0])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2feeee70-0da1-4766-8480-caf19b18f488",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1708.fit.\n: org.apache.spark.SparkException: surrogate cannot be computed. All the values in RecInd,T_prec,S_prec,SThtaq,NH3uM,C14As1,C14A1p,C14As2,C14A2p,DarkAs,DarkAp,MeanAs,MeanAp,LightP,R_Depth,R_TEMP,R_SALINITY,R_SIGMA,R_SVA,R_DYNHT,R_O2,R_O2Sat,R_SIO3,R_PO4,R_NO3,R_NO2,R_NH4,R_CHLA,R_PHAEO,R_PRES,R_SAMP,DIC1,DIC2,TA1,TA2,pH2,pH1 are Null, Nan or missingValue(NaN)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:198)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' Filling in the remaining null rows with the mean of the column and saving this newly filled in dataframe as a new variable '''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bottleTest \u001b[38;5;241m=\u001b[39m \u001b[43mfill_na\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are still\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(bottle\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns and\u001b[39m\u001b[38;5;124m\"\u001b[39m, bottle\u001b[38;5;241m.\u001b[39mcount(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m bottleTest\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mfill_na\u001b[0;34m(df, strategy)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_na\u001b[39m(df, strategy):    \n\u001b[1;32m      5\u001b[0m     imputer \u001b[38;5;241m=\u001b[39m Imputer(\n\u001b[1;32m      6\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m      7\u001b[0m         inputCols\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns, \n\u001b[1;32m      8\u001b[0m         outputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124;03m''' Select the newly created columns with all filled values '''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m new_df\u001b[38;5;241m.\u001b[39mselect([c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m new_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1708.fit.\n: org.apache.spark.SparkException: surrogate cannot be computed. All the values in RecInd,T_prec,S_prec,SThtaq,NH3uM,C14As1,C14A1p,C14As2,C14A2p,DarkAs,DarkAp,MeanAs,MeanAp,LightP,R_Depth,R_TEMP,R_SALINITY,R_SIGMA,R_SVA,R_DYNHT,R_O2,R_O2Sat,R_SIO3,R_PO4,R_NO3,R_NO2,R_NH4,R_CHLA,R_PHAEO,R_PRES,R_SAMP,DIC1,DIC2,TA1,TA2,pH2,pH1 are Null, Nan or missingValue(NaN)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:198)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "''' Filling in the remaining null rows with the mean of the column and saving this newly filled in dataframe as a new variable '''\n",
    "bottleTest = fill_na(bottle, 'mean')\n",
    "\n",
    "print(\"There are still\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottleTest.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc767f-1e7a-4b70-970e-e21da8a031fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Double checking that all null values are filled '''\n",
    "getNullCounts(bottleTest).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14ebf3-0e74-427d-afa8-6d62bf5bda09",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370fcbc-28f2-455d-b6c4-25f0f9b9c89f",
   "metadata": {},
   "source": [
    "> (my personal notes will indented)\n",
    "> 1. tried to use Lasso in spark, turns out it's depricated, use LinearRegression instead\n",
    "> 2. LinearRegression needs a label column (easy)\n",
    "> 3. LinearRegression also needs a feature column (need help determining if this needs to be scaled before using lasso)\n",
    "> \n",
    "> spark lasso: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.mllib.regression.LassoWithSGD.html<br>\n",
    "Warning: **\"Use [pyspark.ml.regression.LinearRegression](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.regression.LinearRegression.html) with elasticNetParam = 1.0. Note the default regParam is 0.01 for LassoWithSGD, but is 0.0 for LinearRegression.\"**\n",
    "> \n",
    "> need to specify a label column in order to use. default is to use a column *named* \"label\" but it *is* possible to have the model look for a specific name. however, renaming columns is something we've done in past PAs and i wanna use it to be like \"HEY-YO WE'RE USING WHAT YOU SHOWED US :D\"\n",
    "> \n",
    "> **(need help here:)** need to create \"features\" column first (did this in PA3, filled NaNs with mean before creating \"features\" column), but not sure if we need to scale it before feeding to lasso AND not sure what alpha \"regParam\" to use (i thiiiiink regParam == alpha ? 😅 i'm actually not sure, but see the raw-cell below for more about which regParam to use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7658788-0a03-4dd7-a0c0-75fe4dbe479a",
   "metadata": {},
   "source": [
    "To better estimate what columns are most important to our model, we use a lasso regression. Because spark LassoWithSGD has depricated, we use LinearRegression with `elasticNetParam = 1.0` for Lasso regression equivalent.\n",
    "\n",
    "In order to use this, `label` and `features` columns need to be specified. We rename `R_CHLA` to `label` and create a features column with VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0fc8b-317e-43ea-9039-d68d55ccfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Rename the target chlorophyll column to 'label' '''\n",
    "bottleTest = bottleTest.withColumnRenamed('R_CHLA', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c802847-9138-4e3a-836c-22ef5560d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating 'features' column '''\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "''' (interm step) make list of column names other than 'label,' AKA make a list of the features '''\n",
    "features = bottleTest.columns\n",
    "features.remove('label')\n",
    "\n",
    "# bottleTest = VectorAssembler(outputCol=\"features_unscaled\").setInputCols(features).transform(bottleTest)\n",
    "bottleTest = VectorAssembler(outputCol=\"features\").setInputCols(features).transform(bottleTest)\n",
    "\n",
    "bottleTest.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7c27da6-a97e-4459-925d-e04ae1219c2e",
   "metadata": {},
   "source": [
    "''' Scaling the features column '''\n",
    "''' NOTE: to use this cell, un-comment-in the line in the above cell and then comment-out the line below it, then run this cell '''\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, inputCol='features_unscaled', outputCol='features')\n",
    "scaledModel = standardScaler.fit(bottleTest)\n",
    "bottleTest = scaledModel.transform(bottleTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68de1e3-9d66-4945-8d2f-f8542445644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Applying lasso regression where regParam= 0.0025 in place of alpha (?) '''\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(elasticNetParam = 1.0, regParam=0.0025, solver=\"normal\", maxIter=1000, standardization=True)\n",
    "model = lr.fit(bottleTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90294013-41b0-462c-be0e-3dccddc4028c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### regParam (alpha?) tinkering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bae0ec-71b9-411d-8572-d4c4bb9d160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"when regParams=\" + str(lr.getRegParam()) + \", model.coefficients=\")\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6b7b6-9411-4381-8714-a6a97e5b3021",
   "metadata": {
    "tags": []
   },
   "source": [
    "> okay i'm not sure how to adjust alpha like we would with a regular non-spark LassoRegression, but changing regParam did change which columns had nonzero coefficients which is what we did for regular Lasso\n",
    "> \n",
    "> all of the model.coefficients align with the six columns `'R_SALINITY','R_DYNHT','R_O2','R_SIO3','R_NO2','R_PHAEO'` but otherwise include at least one more that doesn't appear in all other regParam tests\n",
    ">\n",
    "> | regParam | nonzero coeffs | S-prec? | R_TEMP? | R_SVA? | R_O2Sat? | R_NO3 |\n",
    "> |----------|----------------|---------|---------|--------|----------|-------|\n",
    "> | 0.0      |       17       |   Yes   |   Yes   |   Yes  |    Yes   |  Yes  |\n",
    "> | 0.001    |       13       |   Yes   |   Yes   |        |    Yes   |  Yes  |\n",
    "> | 0.002    |       10       |   Yes   |   Yes   |        |    Yes   |  Yes  |\n",
    "> | 0.003    |       10       |   Yes   |   Yes   |        |    Yes   |  Yes  |\n",
    "> | 0.005    |        9       |   Yes   |   Yes   |        |          |       |\n",
    "> | 0.008    |        8       |         |   Yes   |        |          |       |\n",
    "> | 0.010    |        7       |         |   Yes   |        |          |       |\n",
    "> | 0.013    |        8       |         |   Yes   |   Yes  |          |       |\n",
    "> | 0.015    |        8       |         |   Yes   |   Yes  |          |       |\n",
    "> | 0.018    |        7       |         |         |   Yes  |          |       |\n",
    "> | 0.020    |        7       |         |         |   Yes  |          |       |\n",
    ">\n",
    "> also it looks like scaling doesn't actually change much other than the absolute value of the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcf9e2-c270-45d0-8f48-043542867e2a",
   "metadata": {},
   "source": [
    "##### using the coeff list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf40ca6-1926-434f-8d22-df15fa8cbc70",
   "metadata": {},
   "source": [
    "> here are the \"most useful\" features. i'm not sure if i should be deleting the \"useless\" ones or just dropping nulls from the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ec5ff-84a9-4d00-8643-16ad265786d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Find the \"most useful\" and \"not useful\" features, as according to the lasso regresion when regParam=0.0025 '''\n",
    "\n",
    "coeff = model.coefficients\n",
    "usefulFeatures = []\n",
    "deleteList5 = []\n",
    "\n",
    "print(\"(For-loop of\", len(coeff), \"iterations)\")\n",
    "for i in range(len(coeff)):\n",
    "    if coeff[i] != 0:\n",
    "        usefulFeatures.append(features[i])\n",
    "    else:\n",
    "        deleteList5.append(features[i])\n",
    "\n",
    "print(\"When regParam=\"+str(lr.getRegParam())+\", this lasso model indicates the most useful columns to predicting chlorophyll are:\", usefulFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4702c4-a788-4d7d-bffb-089054303af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Moving back to dataframe before nulls were filled with means, drop the null rows of the \"most useful\" features '''\n",
    "bottle = bottle.dropna(subset=usefulFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333bf35-ecff-4206-bf78-50f4fdb7989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Moving back to dataframe before nulls were filled with means, drop the \"not useful\" features '''\n",
    "bottle = bottle.drop(*deleteList5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470f79d-d0c2-4054-b1a8-7213ea0b8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' View the dimensions and two rows of the resulting dataframe '''\n",
    "print(\"Moving back to dataframe before nulls were filled with means, here are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows\")\n",
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b0dee-783a-4463-b73b-be6883526fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Make sure column values are the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e21c2-f13f-4272-a3d4-8fb858aad8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Check the dtypes of each column '''\n",
    "bottle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a487f4e-e1dc-488c-bdcd-97fdebdf7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Row count, and comparison to original row count '''\n",
    "print(\"There are now\", len(bottle.columns), \"columns and\", bottle.count(), \"rows (which is\", maxRows-bottle.count(), \"less than the original row count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eeecac-6666-4f14-82ab-e8f0acad2871",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summary Statistics and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7833e-cdf2-40c6-9034-68580c4792d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' See the detailed numerical description of the current dataframe '''\n",
    "bottle.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ca65c-c9eb-4914-a022-773b080725d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getNullCounts(bottle).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251cc71-1429-431e-a02d-be9dcbf6cc97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2351ad-7933-42e5-941e-8060272483b3",
   "metadata": {},
   "source": [
    "#### Which factors are chlorophyll levels most dependent on?\n",
    "#### Can we use these factors as features in a model that accurately predict chlorophyll levels in a marine ecosystem?\n",
    "#### Can these few, simple, measurable factors we found indicate whether an ocean ecosystem is healthy and sustainable based on the chlorophyll abundance measured?\n",
    "#### Can we predict chlorophyll abundance without having to measure it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6779b1-c4c1-4527-b8f3-e554a5b67756",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Selection and Target Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d2dff-7ab8-4b77-ab04-f7e155558db2",
   "metadata": {},
   "source": [
    "#### Distribution of data for cholorophyll levels, summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a00435-d31b-468e-b2a2-de4e563ed908",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle.agg(min('R_CHLA'), max('R_CHLA'), mean('R_CHLA'), stddev('R_CHLA'), count('R_CHLA'), skewness('R_CHLA')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8c427-eedd-4d30-9c0d-64c8945816be",
   "metadata": {},
   "source": [
    "##### Looking at this summary data of our chlorophyll levels, the data is highly positively skewed. This is displayed in our skewness value but also the difference between the maximum value and the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9025425-d4b7-46ac-959c-ce50eae390d7",
   "metadata": {},
   "source": [
    "#### Define cholorophyll category levels: low, medium, high?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f502847-fec1-4fbb-9886-ef0ce8a3c79e",
   "metadata": {},
   "source": [
    "##### As displayed in Figure 4 of this article,https://www.nature.com/scitable/knowledge/library/the-biological-productivity-of-the-ocean-70631104/, the chlorophyll concentrations in the ocean can be split into three category levels, low, medium, and high. Our low category range will be between 0 > x > .1ug/l, medium range will be .1 >= x > 1ug/l, and high concentrations will be x >= 1ug/l."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e5a20-33d3-44d4-af6b-99d598adf662",
   "metadata": {},
   "source": [
    "#### Map cholorophyll levels to defined categories and output graphs/summary stats and Add target column with category label to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860e212-d6f5-4c6c-8457-22fbc18cb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowThreshold = .1\n",
    "medThreshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b454b-10ea-4fd5-94ba-f7d6a5fc2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle = bottle.withColumn(\n",
    "    'Target',\n",
    "     when((col(\"R_CHLA\") < lowThreshold), 0)\\\n",
    "    .when((col(\"R_CHLA\").between(lowThreshold, medThreshold)), 1)\\\n",
    "    .when((col(\"R_CHLA\") > medThreshold), 2)\\\n",
    "    .otherwise(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dffcb2-a63b-44ac-b0a0-097c8eab626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle.groupby('Target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c2f01-aa1e-4b4e-a85c-02a962a12bc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PCA columns that are duplicates and analyse if we can get rid of null columns without removal of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938135b-6a90-435f-bbc0-dba3bc60d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b783cd-f152-44a4-8a98-42debd8056ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['col1', 'col2', 'correlation']\n",
    "schema = StructType([\n",
    "  StructField('col1', StringType(), False),\n",
    "  StructField('col2', StringType(), False),\n",
    "  StructField('correlation', IntegerType(), False)\n",
    "  ])\n",
    "highCorrs = spark.createDataFrame(spark.sparkContext.emptyRDD() ,schema)\n",
    "rangeColumns = range(len(bottle.columns))\n",
    "for i in rangeColumns:\n",
    "    for j in rangeColumns:\n",
    "        if(i != j):\n",
    "            correlation = bottle.corr(bottle.columns[i], bottle.columns[j])\n",
    "            if(correlation > .9):\n",
    "                newCorr = spark.createDataFrame([(bottle.columns[i], bottle.columns[j], correlation)], columns)\n",
    "                highCorrs = highCorrs.union(newCorr)\n",
    "            else: \n",
    "                 continue;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9016a5d-90db-4b9d-a171-2fa54dd6f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "highCorrs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb8cdc-151a-45b1-b965-99691b0449b7",
   "metadata": {},
   "source": [
    "Because R_PO4 has 998 nulls and is highly correlated with R_NO3 and R_SIO3, we will drop this column with little reduction in variance. R_SVA also contains nulls and is highly correlated with R_Temp, so we will drop this column as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e16fb5-856f-4505-bd5d-dd5ad33d1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle = bottle.drop('R_O2')\n",
    "bottle.withColumnRenamed('R_O2Sat', 'R_O2, R_O2Sat')\n",
    "bottle = bottle.drop('R_SIO3')\n",
    "bottle.withColumnRenamed('R_NO3', 'R_SIO3, R_NO3')\n",
    "bottle = bottle.drop('R_CHLA')\n",
    "bottle.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eee978-81f7-4247-8274-ed0bfee2475e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7281a0-e86c-4bca-9538-b6b3241ead44",
   "metadata": {},
   "source": [
    "#### Split data into 80/10/10 train, test, validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a51dc1-e61c-472e-994a-b1c183272ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking schema of data again. Using bottleTest df instead of 'bottle' because it has no null values and used vector assembler to create feature vector.\n",
    "bottle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba60151-2cd7-4c87-ab69-8b3757a64d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bottle.columns\n",
    "features.remove('Target')\n",
    "bottle = VectorAssembler(outputCol=\"features_unscaled\").setInputCols(features).transform(bottle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d1480-fdd0-4157-a713-9b9b6ff41bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f46674-0153-4411-a2e3-79261acdc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train, test, validation = bottle.randomSplit([0.80, 0.10, 0.10], seed=seed)\n",
    "print('Train dataset count:', train.count())\n",
    "print('Test dataset count:', test.count())\n",
    "print('Validation dataset count:', validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed6ea1-6067-4248-b944-75cd0fde64dd",
   "metadata": {},
   "source": [
    "#### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c0421-b823-482e-9ddf-cb52f5df883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol='features_unscaled', outputCol='features')\n",
    "ss = standardScaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76382b4a-1f38-43e5-a81b-3d7acc3948b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('StandardScaler Means:', ss.mean)\n",
    "print('StandardScaler StDevs:', ss.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e613a-ed5e-42be-9908-49bb8c9e5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainscaled = ss.transform(train)\n",
    "testscaled = ss.transform(test)\n",
    "valscaled = ss.transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f545e17-8f35-4c13-b6d5-0ccf67c6fe7b",
   "metadata": {},
   "source": [
    "#### Check for Imblance in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efa3c7-672b-41e5-a80b-eb927bf7f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainscaled.groupby('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c31ca-95c6-4481-bb14-d5765c483c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "testscaled.groupby('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eed65a-ce02-484e-a51a-d0501245f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "valscaled.groupby('target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943198d-21eb-4109-a15d-d9d698002c4e",
   "metadata": {},
   "source": [
    "The datasets are well balanced with proportional amounts of each feature in each set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c889c47-658e-4d0c-925c-56363eb63b30",
   "metadata": {},
   "source": [
    "### Logistic Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa83b8-82fa-4685-b07f-a7b5eb9d0dd9",
   "metadata": {},
   "source": [
    "#### Cross Validation, Check to see if data can be used to predict labels, Train and Validation Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046004f5-a713-424d-8a5b-b3d0cd3eae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d15b6-d832-4d35-ab5c-a47f362f2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tuning\n",
    "# regParams = [0.001, 0.002, 0.003, 0.004]\n",
    "# elasticNetParams = [0.5, 0.6, 0.7]\n",
    "\n",
    "# for r in regParams:\n",
    "#     for e in elasticNetParams:\n",
    "#         lr = LogisticRegression(featuresCol='features', labelCol='Target', predictionCol='prediction', maxIter=10, regParam=r, elasticNetParam=e)\n",
    "#         lrModel = lr.fit(trainscaled)\n",
    "#         print('regParam', r, 'elasticNetParam', e, 'accuracy', lrModel.summary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000f097-aa4c-4b92-a832-07966729f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol='Target', predictionCol='prediction', \\\n",
    "                        maxIter=10, regParam=0.001, elasticNetParam=0.6)\n",
    "lrModel = lr.fit(trainscaled)\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "    \n",
    "print('-------------------------------------------')\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFalse Positive Rate: %s\\nTrue Positive Rate: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653cb6b-61b7-4620-b67c-5bb8faa38686",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictVal = lrModel.evaluate(valscaled)\n",
    "accuracy = predictVal.accuracy\n",
    "falsePositiveRate = predictVal.weightedFalsePositiveRate\n",
    "truePositiveRate = predictVal.weightedTruePositiveRate\n",
    "fMeasure = predictVal.weightedFMeasure()\n",
    "precision = predictVal.weightedPrecision\n",
    "recall = predictVal.weightedRecall\n",
    "\n",
    "print(\"Accuracy: %s\\nFalse Positive Rate: %s\\nTrue Positive Rate: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c46b8b-4a31-4842-9247-373e3c218377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Logistic Regression with 5 fold Cross Validation doesn't work yet\n",
    "\n",
    "# lr = LogisticRegression(featuresCol='features', labelCol='Target', predictionCol='Prediction')\n",
    "\n",
    "# # lrModel = lr.fit(trainscaled)\n",
    "\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(lr.regParam, [0.001, 0.01, 0.1, 1.0, 10.0])\n",
    "#              .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "#              .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "#              .build())\n",
    "\n",
    "# evaluator = MulticlassClassificationEvaluator(predictionCol='Prediction')\n",
    "\n",
    "# cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=2)\n",
    "# cvmodel = cv.fit(trainscaled)\n",
    "\n",
    "# predictTrain=cvmodel.transform(trainscaled)\n",
    "# predictVal=cvmodel.transform(valscaled)\n",
    "\n",
    "# print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predictTrain)))\n",
    "# print(\"The area under ROC for validation set is {}\".format(evaluator.evaluate(predictVal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01afc4-f7e7-4543-aef6-60d61de295c7",
   "metadata": {},
   "source": [
    "### AdaBoost Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd8d1c-fa8a-478a-9015-3985eac57ed2",
   "metadata": {},
   "source": [
    "#### Can we increase accuracy from previous steps? Can we figure out which features matter the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b28f3-59ac-478b-a548-6b8d54fc0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6864d-5b5d-4931-b00f-3e8f861a22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"Target\", seed=seed)\n",
    "dtModel = dt.fit(trainscaled)\n",
    "dtModel.setFeaturesCol(\"features\")\n",
    "trainResult = dtModel.transform(trainscaled)\n",
    "valResult =  dtModel.transform(valscaled)\n",
    "print(trainResult.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd736015-16d7-4ce2-a070-675e01a686c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    correct = df.filter(df[\"Target\"] == df[\"prediction\"]).count()\n",
    "    incorrect = df.filter(df[\"Target\"] != df[\"prediction\"]).count()\n",
    "    totalRows = df.count()\n",
    "    accuracy = correct/totalRows\n",
    "    error = incorrect/totalRows\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Error Rate: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04787066-4049-463e-a406-d11c09ab25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(trainResult)\n",
    "calculate_accuracy(valResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d70d62-0615-4bcc-a44c-bd5dee0c4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Depth:\", dtModel.depth)\n",
    "print(\"Model Number of Nodes:\", dtModel.numNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e9e8a-a4c2-4f8b-b846-b6a6362015c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature importance', dtModel.featureImportances)\n",
    "print('feature col', dtModel.featuresCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0d45a-50f5-4110-b877-9ab2de7d9d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baa228f7-a545-4030-93f1-3181866d9057",
   "metadata": {},
   "source": [
    "# Stop spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49137ca0-7293-4df6-b5c2-5e8f2d855c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
